#
# Python program to open and process a PDF file, extracting
# all numeric values from the document. The goal is to tally
# the first significant (non-zero) digit of each numeric
# value, and save the results to a text file. This will
# allow checking to see if the results follow Benford's Law,
# a common method for detecting fraud in numeric data.
#
# https://en.wikipedia.org/wiki/Benford%27s_law
# https://chance.amstat.org/2021/04/benfords-law/

import json
import boto3
import os
import uuid
import base64
import pathlib
import datatier
import urllib.parse
import string

from configparser import ConfigParser
from pypdf import PdfReader

def lambda_handler(event, context):
  try:
    print("**STARTING**")
    print("**lambda: proj03_compute**")
    
    # 
    # in case we get an exception, initial this filename
    # so we can write an error message if need be:
    #
    bucketkey_results_file = ""
    
    #
    # setup AWS based on config file:
    #
    config_file = 'benfordapp-config.ini'
    os.environ['AWS_SHARED_CREDENTIALS_FILE'] = config_file
    
    configur = ConfigParser()
    configur.read(config_file)
    
    #
    # configure for S3 access:
    #
    s3_profile = 's3readwrite'
    boto3.setup_default_session(profile_name=s3_profile)
    
    bucketname = configur.get('s3', 'bucket_name')
    
    s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucketname)
    
    #
    # configure for RDS access
    #
    rds_endpoint = configur.get('rds', 'endpoint')
    rds_portnum = int(configur.get('rds', 'port_number'))
    rds_username = configur.get('rds', 'user_name')
    rds_pwd = configur.get('rds', 'user_pwd')
    rds_dbname = configur.get('rds', 'db_name')
    
    #
    # this function is event-driven by a PDF being
    # dropped into S3. The bucket key is sent to 
    # us and obtain as follows:
    #
    bucketkey = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    
    print("bucketkey:", bucketkey)
      
    extension = pathlib.Path(bucketkey).suffix
    
    if extension != ".pdf" : 
      raise Exception("expecting S3 document to have .pdf extension")
    
    bucketkey_results_file = bucketkey[0:-4] + ".txt"
    
    print("bucketkey results file:", bucketkey_results_file)
      
    #
    # download PDF from S3 to LOCAL file system:
    #
    print("**DOWNLOADING '", bucketkey, "'**")

    local_pdf = "/tmp/data.pdf"
    
    bucket.download_file(bucketkey, local_pdf)

    #
    # open LOCAL pdf file:
    #
    print("**PROCESSING local PDF**")
    
    try: 
      reader = PdfReader(local_pdf)
      number_of_pages = len(reader.pages)
    except Exception as e:
      print("Error processing:", e)
      raise

    #
    # open connection to the database:
    #
    print("**Opening DB connection**")
    
    dbConn = datatier.get_dbConn(rds_endpoint, rds_portnum, rds_username, rds_pwd, rds_dbname)
    cursor = dbConn.cursor()

    # Start job processing - update to 'processing' status
    try:
        update_query = """
        UPDATE jobs
        SET status = 'processing - starting'
        WHERE datafilekey = %s
        """
        cursor = dbConn.cursor()
        cursor.execute(update_query, (bucketkey,))
        dbConn.commit()
        print("Status updated to 'processing - starting'")
    except Exception as e:
        print("Error updating to 'processing - starting':", e)
        raise

    digit_counts = {
      '0': 0,
      '1': 0,
      '2': 0,
      '3': 0,
      '4': 0,
      '5': 0,
      '6': 0,
      '7': 0,
      '8': 0,
      '9': 0
    }

    #
    # for each page, extract text, split into words,
    # and see which words are numeric values:
    #
    for i in range(0, number_of_pages):
      page = reader.pages[i]
      text = page.extract_text()
      words = text.split()
      print("** Page", i+1, ", text length", len(text), ", num words", len(words))
      for word in words:
        word = word.translate(str.maketrans('', '', string.punctuation))
        if word.isnumeric():
          leading_digit = word.lstrip('0')
          
          if leading_digit and leading_digit[0] in digit_counts:
              digit_counts[leading_digit[0]] += 1

      # now that page has been processed, let's update database to
      # show progress...

      total_pages = number_of_pages 

      # Processing each page
      for page in range(number_of_pages):
          try:
              status_message = f"processing - page {page + 1} of {number_of_pages} completed"
              update_query = """
              UPDATE jobs
              SET status = %s
              WHERE datafilekey = %s
              """
              cursor = dbConn.cursor()
              cursor.execute(update_query, (status_message, bucketkey))
              dbConn.commit()
              print(f"Status updated to '{status_message}' for datafilekey:", bucketkey)
          except Exception as e:
              print("Error updating processing status:", e)
        

    # Final update after processing
    try:
        update_query = """
        UPDATE jobs
        SET status = 'completed'
        WHERE datafilekey = %s
        """
        cursor = dbConn.cursor()
        cursor.execute(update_query, (bucketkey,))
        dbConn.commit()
        print("Job status updated to 'completed' with datafilekey:", bucketkey)
    except Exception as e:
        print("Error updating job status to 'completed':", e)
    

    #
    # analysis complete, write the results to local results file:
    #
    local_results_file = "/tmp/results.txt"

    print("local results file:", local_results_file)

    outfile = open(local_results_file, "w")
    outfile.write("**RESULTS**\n")
    outfile.write(str(number_of_pages))
    outfile.write(" pages\n")

    with open(local_results_file, "w") as outfile:
        outfile.write("**RESULTS**\n")
        outfile.write(f"{number_of_pages} pages\n")
        
        for digit in range(0, 10):
            count = digit_counts.get(str(digit), 0)  
            outfile.write(f"{digit} {count}\n")
    

    outfile.close()
    
    #
    # upload the results file to S3:
    #
    print("**UPLOADING to S3 file", bucketkey_results_file, "**")

    bucket.upload_file(local_results_file,
                       bucketkey_results_file,
                       ExtraArgs={
                         'ACL': 'public-read',
                         'ContentType': 'text/plain'
                       })
    
    # 
    # The last step is to update the database to change
    # the status of this job, and store the results
    # bucketkey for download:

    update_query = """
        UPDATE jobs
        SET status = 'completed', resultsfilekey = %s
        WHERE datafilekey = %s
    """

    try:
        cursor = dbConn.cursor()
        cursor.execute(update_query, (bucketkey_results_file, bucketkey))
        dbConn.commit()
        cursor.close()
        print("Job status updated to 'completed' with resultsfilekey.")
    except Exception as e:
        print("Error updating job status and resultsfilekey:", e)

    #
    # done!
    #
    # respond in an HTTP-like way, i.e. with a status
    # code and body in JSON format:
    #
    print("**DONE, returning success**")
    
    return {
      'statusCode': 200,
      'body': json.dumps("success")
    }
    
  #
  # on an error, try to upload error message to S3:
  #
  except Exception as err:
    print("**ERROR**")
    print(str(err))
    
    local_results_file = "/tmp/results.txt"
    outfile = open(local_results_file, "w")

    outfile.write(str(err))
    outfile.write("\n")
    outfile.close()
    
    if bucketkey_results_file == "": 
      #
      # we can't upload the error file:
      #
      pass
    else:
      # 
      # upload the error file to S3
      #
      print("**UPLOADING**")
      #
      bucket.upload_file(local_results_file,
                         bucketkey_results_file,
                         ExtraArgs={
                           'ACL': 'public-read',
                           'ContentType': 'text/plain'
                         })

    #
    # update jobs row in database:
    #
    # TODO #8 of 8: open connection, update job in database
    # to reflect that an error has occurred. The job is 
    # identified by the bucketkey --- datafilekey in the 
    # table. Set the status column to 'error' and set the
    # resultsfilekey column to the contents of the variable
    # bucketkey_results_file.
    #
    dbConn = datatier.get_dbConn(rds_endpoint, rds_portnum, rds_username, rds_pwd, rds_dbname)
    #
    # ???
    #
    with dbConn.cursor() as cursor:
        sql = "UPDATE jobs SET status = %s, resultsfilekey = %s WHERE datafilekey = %s"
        cursor.execute(sql, ("error", bucketkey_results_file, bucketkey))
        dbConn.commit()
    #
    # done, return:
    #    
    return {
      'statusCode': 500,
      'body': json.dumps(str(err))
    }
